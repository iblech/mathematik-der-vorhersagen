=== Tag 0, abends

* Vorstellung
* Orgakram (u.a. Dokuforge)
* Motivation
* Mission/Plan: Videos und fertiges Ziffernprogramm

https://www.youtube.com/watch?v=MzJ0CytAsec
Windows Vista Speech Recognition Tested - Perl Scripting

https://www.youtube.com/watch?v=M1ONXea0mXg
Hound Internal Demo

https://www.youtube.com/watch?v=zsVsUvx8ieo
NVIDIA CES 2015 press conference: DRIVE PX Computer Vision (part 7)

https://www.youtube.com/watch?v=qv6UVOQ0F44
MarI/O - Machine Learning for Video Games


=== Tag 1: Analysis-Bootcamp

* Definition der Ableitung im Eindimensionalen
* Zwei Interpretationen: Steigung und lokale Approximation
* Ableitungsregeln, insbesondere Kettenregel
* Python: Gradientenabstieg

* Ableitung im Mehrdimensionalen
* Kettenregel
* Python: Mehrdimensionaler Gradientenabstieg


=== Tag 2

Python in Ruhe:
* Variablen und for/range. Damit Zahlen von 1 bis 100 summieren, Quadratzahlen
  summieren.
* if. Damit Primzahlen bis 100 finden, Collatz-Vermutung testen.
* Listen. Damit Liste der ersten 100 Fibonacci-Zahlen erstellen.

Danach Gradientenabstieg mit ansteigender Komplexität:
1D/2D/nD, Gradientauswertung in ausgelagerter Funktion, ...


=== Tag 3, nur vormittags: Lineare Regression

* Lineare Regression, zunächst aber noch mit Gradientenabstieg statt
  Normalengleichung

Sei f(t; a) sowas wie P(x(t) = a).

=== Tag 4, nur nachmittags: Lineare Regression und neuronale Netze

* jetzt mit Herleitung und Programmierung der Normalengleichung
* Einstieg in neuronale Netze

Das Integral über a von f_2(t; a) x'(t) ist Null, die Normalisierung bleibt
also erhalten.

=== Tag 5: Neuronale Netze

* Neuronale Netze mit einer einzigen Zwischenschicht, die aber beliebig
  viele Neuronen enthalten darf
* ???


=== Tag 6: Markov-Ketten und Rotationsvorbereitung

* Kurze Einführung in Markov-Ketten: Wie funktionieren sie? Beispiele.
* Wie zieht man aus Wahrscheinlichkeitsverteilungen?
* Programmieren in Python


=== Tag 7, nur nachmittags: Neuronale Netze

* XOR in Matrixschreibweise
* XOR-Training


=== Tag 8: Neuronale Netze und Projekte

* Fertigstellung der XOR-Trainings-Programme
* Weitere Entwicklungen bei neuronalen Netzen:
  * Stochastischer Gradientenabstieg
  * tanh-Neuronen
  * (Cross-Entropy-Kostenfunktion)
  * L2-Regularisierung um Overfitting zu begegnen
  * Convolutional Neural Networks
* Visualisierung von neuronalen Netzen und "Träume" von neuronalen Netzen
  http://googleresearch.blogspot.de/2015/06/inceptionism-going-deeper-into-neural.html
  http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf (Zeiler/Fergus:
  Visualizing and Understanding Convolutional Networks)

* Nachmittag???
