=== Tag 0, abends

* Vorstellung
* Orgakram (u.a. Dokuforge)
* Motivation
* Mission/Plan: Videos und fertiges Ziffernprogramm

https://www.youtube.com/watch?v=MzJ0CytAsec
Windows Vista Speech Recognition Tested - Perl Scripting

https://www.youtube.com/watch?v=M1ONXea0mXg
Hound Internal Demo

https://www.youtube.com/watch?v=zsVsUvx8ieo
NVIDIA CES 2015 press conference: DRIVE PX Computer Vision (part 7)

https://www.youtube.com/watch?v=qv6UVOQ0F44
MarI/O - Machine Learning for Video Games


=== Tag 1: Analysis-Bootcamp

* Definition der Ableitung im Eindimensionalen
* Zwei Interpretationen: Steigung und lokale Approximation
* Ableitungsregeln, insbesondere Kettenregel
* Python: Gradientenabstieg

* Ableitung im Mehrdimensionalen
* Kettenregel
* Python: Mehrdimensionaler Gradientenabstieg


=== Tag 2

Python in Ruhe:
* Variablen und for/range. Damit Zahlen von 1 bis 100 summieren, Quadratzahlen
  summieren.
* if. Damit Primzahlen bis 100 finden, Collatz-Vermutung testen.
* Listen. Damit Liste der ersten 100 Fibonacci-Zahlen erstellen.

Danach Gradientenabstieg mit ansteigender Komplexität:
1D/2D/nD, Gradientauswertung in ausgelagerter Funktion, ...


=== Tag 3, nur vormittags: Lineare Regression

* Lineare Regression, zunächst aber noch mit Gradientenabstieg statt
  Normalengleichung

Sei f(t; a) sowas wie P(x(t) = a).

=== Tag 4, nur nachmittags: Lineare Regression und neuronale Netze

* jetzt mit Herleitung und Programmierung der Normalengleichung
* Einstieg in neuronale Netze

Das Integral über a von f_2(t; a) x'(t) ist Null, die Normalisierung bleibt
also erhalten.
